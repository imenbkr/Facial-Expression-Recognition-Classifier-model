{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/imenbakir/facial-expression-recognition-classifier-model?scriptVersionId=118618459\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","id":"c04eca9e","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-02-02T20:11:14.665854Z","iopub.status.busy":"2023-02-02T20:11:14.66543Z","iopub.status.idle":"2023-02-02T20:11:14.693852Z","shell.execute_reply":"2023-02-02T20:11:14.6926Z","shell.execute_reply.started":"2023-02-02T20:11:14.665772Z"},"papermill":{"duration":0.009372,"end_time":"2023-02-08T22:36:42.880954","exception":false,"start_time":"2023-02-08T22:36:42.871582","status":"completed"},"tags":[]},"source":["<h1 style=\"padding:10px;background-color:#C7A9D4;margin:0;color:white;font-family:newtimeroman;font-size:300%;text-align:center;border-radius: 15px 50px;overflow:hidden;font-weight:500\"> Facial Expression Recognition Classifier model </h1>"]},{"cell_type":"markdown","id":"d302f322","metadata":{"papermill":{"duration":0.007814,"end_time":"2023-02-08T22:36:42.896972","exception":false,"start_time":"2023-02-08T22:36:42.889158","status":"completed"},"tags":[]},"source":["## Importing libraries"]},{"cell_type":"code","execution_count":1,"id":"44d1657b","metadata":{"execution":{"iopub.execute_input":"2023-02-08T22:36:42.915145Z","iopub.status.busy":"2023-02-08T22:36:42.914379Z","iopub.status.idle":"2023-02-08T22:36:48.548515Z","shell.execute_reply":"2023-02-08T22:36:48.547314Z"},"papermill":{"duration":5.646339,"end_time":"2023-02-08T22:36:48.551305","exception":false,"start_time":"2023-02-08T22:36:42.904966","status":"completed"},"tags":[]},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","\n","import joblib\n","\n","from tensorflow.keras.utils import plot_model\n","from keras.models import Model\n","from keras.layers import Input, Dense, Flatten, Dropout, BatchNormalization\n","from keras.layers.convolutional import Conv2D\n","from keras.layers.pooling import MaxPooling2D\n","from keras.layers.merge import concatenate\n","from tensorflow.keras.optimizers import Adam, SGD\n","from keras.regularizers import l1, l2\n","from matplotlib import pyplot as plt\n","from sklearn.metrics import confusion_matrix"]},{"cell_type":"markdown","id":"810ea50c","metadata":{"papermill":{"duration":0.008722,"end_time":"2023-02-08T22:36:48.570121","exception":false,"start_time":"2023-02-08T22:36:48.561399","status":"completed"},"tags":[]},"source":["### Dataset\n","\n","The dataset [fer-2013](#https://www.kaggle.com/datasets/ashishpatel26/facial-expression-recognitionferchallenge) has 48*48 pixels gray-scale images of faces along with their emotion labels.\n","\n","This dataset contains 7 Emotions :- (0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral)\n","\n","This dataset contains 3 columns, <a> <span style=\"color:blue;font-weight:bold\">emotion, pixels and Usage </span></a>. Emotion column contains integer encoded emotions and pixels column contains pixels in the form of a string seperated by spaces, and usage tells if data is made for training or testing purpose."]},{"cell_type":"code","execution_count":2,"id":"bd8a256f","metadata":{"execution":{"iopub.execute_input":"2023-02-08T22:36:48.587515Z","iopub.status.busy":"2023-02-08T22:36:48.586978Z","iopub.status.idle":"2023-02-08T22:36:54.272409Z","shell.execute_reply":"2023-02-08T22:36:54.271463Z"},"papermill":{"duration":5.696706,"end_time":"2023-02-08T22:36:54.274834","exception":false,"start_time":"2023-02-08T22:36:48.578128","status":"completed"},"tags":[]},"outputs":[],"source":["df = pd.read_csv('/kaggle/input/facial-expression-recognitionferchallenge/fer2013/fer2013/fer2013.csv')"]},{"cell_type":"code","execution_count":3,"id":"73504002","metadata":{"execution":{"iopub.execute_input":"2023-02-08T22:36:54.294341Z","iopub.status.busy":"2023-02-08T22:36:54.292797Z","iopub.status.idle":"2023-02-08T22:36:54.312273Z","shell.execute_reply":"2023-02-08T22:36:54.311415Z"},"papermill":{"duration":0.030847,"end_time":"2023-02-08T22:36:54.314308","exception":false,"start_time":"2023-02-08T22:36:54.283461","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>emotion</th>\n","      <th>pixels</th>\n","      <th>Usage</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n","      <td>Training</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n","      <td>Training</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n","      <td>Training</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n","      <td>Training</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>6</td>\n","      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n","      <td>Training</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   emotion                                             pixels     Usage\n","0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n","1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n","2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n","3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n","4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["df.head()"]},{"cell_type":"markdown","id":"c3dfec53","metadata":{"papermill":{"duration":0.0079,"end_time":"2023-02-08T22:36:54.330475","exception":false,"start_time":"2023-02-08T22:36:54.322575","status":"completed"},"tags":[]},"source":["### Splitting data into training and testing sets"]},{"cell_type":"code","execution_count":4,"id":"17728802","metadata":{"execution":{"iopub.execute_input":"2023-02-08T22:36:54.347994Z","iopub.status.busy":"2023-02-08T22:36:54.347716Z","iopub.status.idle":"2023-02-08T22:37:10.985169Z","shell.execute_reply":"2023-02-08T22:37:10.984058Z"},"papermill":{"duration":16.648993,"end_time":"2023-02-08T22:37:10.987565","exception":false,"start_time":"2023-02-08T22:36:54.338572","status":"completed"},"tags":[]},"outputs":[],"source":["X_train = []\n","y_train = []\n","X_test = []\n","y_test = []\n","for index, row in df.iterrows():\n","    k = row['pixels'].split(\" \")\n","    if row['Usage'] == 'Training':\n","        X_train.append(np.array(k))\n","        y_train.append(row['emotion'])\n","    elif row['Usage'] == 'PublicTest':\n","        X_test.append(np.array(k))\n","        y_test.append(row['emotion'])"]},{"cell_type":"code","execution_count":5,"id":"d0eafa9b","metadata":{"execution":{"iopub.execute_input":"2023-02-08T22:37:11.006354Z","iopub.status.busy":"2023-02-08T22:37:11.005423Z","iopub.status.idle":"2023-02-08T22:37:11.013326Z","shell.execute_reply":"2023-02-08T22:37:11.012477Z"},"papermill":{"duration":0.019195,"end_time":"2023-02-08T22:37:11.015326","exception":false,"start_time":"2023-02-08T22:37:10.996131","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["array(['70', '80', '82', ..., '106', '109', '82'], dtype='<U3')"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["X_train[0]"]},{"cell_type":"markdown","id":"fea25322","metadata":{"papermill":{"duration":0.008083,"end_time":"2023-02-08T22:37:11.031587","exception":false,"start_time":"2023-02-08T22:37:11.023504","status":"completed"},"tags":[]},"source":["We notice that the integer in the training and testing datasets are in the form of integers, we have to convert them to integer"]},{"cell_type":"code","execution_count":6,"id":"4c6ddc56","metadata":{"execution":{"iopub.execute_input":"2023-02-08T22:37:11.050394Z","iopub.status.busy":"2023-02-08T22:37:11.049021Z","iopub.status.idle":"2023-02-08T22:37:29.717265Z","shell.execute_reply":"2023-02-08T22:37:29.716274Z"},"papermill":{"duration":18.679863,"end_time":"2023-02-08T22:37:29.719596","exception":false,"start_time":"2023-02-08T22:37:11.039733","status":"completed"},"tags":[]},"outputs":[],"source":["X_train = np.array(X_train, dtype = 'uint8')\n","y_train = np.array(y_train, dtype = 'uint8')\n","X_test = np.array(X_test, dtype = 'uint8')\n","y_test = np.array(y_test, dtype = 'uint8')"]},{"cell_type":"markdown","id":"352ead58","metadata":{"papermill":{"duration":0.008055,"end_time":"2023-02-08T22:37:29.736205","exception":false,"start_time":"2023-02-08T22:37:29.72815","status":"completed"},"tags":[]},"source":["y_test, y_train contains 1D integer encoded labels, we need to connect them into categorical data for efficient training."]},{"cell_type":"code","execution_count":7,"id":"c855518b","metadata":{"execution":{"iopub.execute_input":"2023-02-08T22:37:29.754525Z","iopub.status.busy":"2023-02-08T22:37:29.753622Z","iopub.status.idle":"2023-02-08T22:37:29.759233Z","shell.execute_reply":"2023-02-08T22:37:29.758399Z"},"papermill":{"duration":0.016694,"end_time":"2023-02-08T22:37:29.76118","exception":false,"start_time":"2023-02-08T22:37:29.744486","status":"completed"},"tags":[]},"outputs":[],"source":["from tensorflow.keras.utils import to_categorical\n","y_train= to_categorical(y_train, num_classes=7)\n","y_test = to_categorical(y_test, num_classes=7)"]},{"cell_type":"markdown","id":"5d452353","metadata":{"papermill":{"duration":0.007903,"end_time":"2023-02-08T22:37:29.7776","exception":false,"start_time":"2023-02-08T22:37:29.769697","status":"completed"},"tags":[]},"source":["we have 7 classes to classify."]},{"cell_type":"markdown","id":"f9a304d1","metadata":{"papermill":{"duration":0.007922,"end_time":"2023-02-08T22:37:29.793696","exception":false,"start_time":"2023-02-08T22:37:29.785774","status":"completed"},"tags":[]},"source":["### Reshaping Data\n","\n","convert the data in the form of a 4d tensor __(row_num, width, height, channel)__ for training purposes."]},{"cell_type":"code","execution_count":8,"id":"ea5deabe","metadata":{"execution":{"iopub.execute_input":"2023-02-08T22:37:29.811577Z","iopub.status.busy":"2023-02-08T22:37:29.810795Z","iopub.status.idle":"2023-02-08T22:37:29.81553Z","shell.execute_reply":"2023-02-08T22:37:29.814687Z"},"papermill":{"duration":0.015594,"end_time":"2023-02-08T22:37:29.817466","exception":false,"start_time":"2023-02-08T22:37:29.801872","status":"completed"},"tags":[]},"outputs":[],"source":["X_train = X_train.reshape(X_train.shape[0], 48, 48, 1)\n","X_test = X_test.reshape(X_test.shape[0], 48, 48, 1)"]},{"cell_type":"markdown","id":"64843345","metadata":{"papermill":{"duration":0.00797,"end_time":"2023-02-08T22:37:29.833636","exception":false,"start_time":"2023-02-08T22:37:29.825666","status":"completed"},"tags":[]},"source":["we put 1 so the data is grayscaled"]},{"cell_type":"markdown","id":"c4e168a7","metadata":{"papermill":{"duration":0.008005,"end_time":"2023-02-08T22:37:29.849874","exception":false,"start_time":"2023-02-08T22:37:29.841869","status":"completed"},"tags":[]},"source":["### Image Augmentation for Facial Emotion Detection\n","\n","\n","Image data augmentation is the process of generating new transformed versions of images from the given image dataset to increase its diversity. This is helpful when we are given a data-set with very few data samples. In case of Deep Learning, this situation is bad as the model tends to over-fit when we train it on limited number of data samples.\n","\n","To a computer, images are just a 2-dimensional array of numbers. These numbers represent pixel values, which you can tweak in many ways to generate new, augmented images.\n","\n","This can be done using __ImageDataGenetrator__ provided by Keras. "]},{"cell_type":"code","execution_count":9,"id":"130c039b","metadata":{"execution":{"iopub.execute_input":"2023-02-08T22:37:29.867874Z","iopub.status.busy":"2023-02-08T22:37:29.867083Z","iopub.status.idle":"2023-02-08T22:37:29.872235Z","shell.execute_reply":"2023-02-08T22:37:29.871444Z"},"papermill":{"duration":0.01605,"end_time":"2023-02-08T22:37:29.874128","exception":false,"start_time":"2023-02-08T22:37:29.858078","status":"completed"},"tags":[]},"outputs":[],"source":["from keras.preprocessing.image import ImageDataGenerator \n","datagen = ImageDataGenerator( \n","    rescale=1./255,\n","    rotation_range = 10,\n","    horizontal_flip = True,\n","    width_shift_range=0.1,\n","    height_shift_range=0.1,\n","    fill_mode = 'nearest')"]},{"cell_type":"code","execution_count":10,"id":"c8395a8e","metadata":{"execution":{"iopub.execute_input":"2023-02-08T22:37:29.891949Z","iopub.status.busy":"2023-02-08T22:37:29.891167Z","iopub.status.idle":"2023-02-08T22:37:30.05025Z","shell.execute_reply":"2023-02-08T22:37:30.049266Z"},"papermill":{"duration":0.170493,"end_time":"2023-02-08T22:37:30.052808","exception":false,"start_time":"2023-02-08T22:37:29.882315","status":"completed"},"tags":[]},"outputs":[],"source":["testgen = ImageDataGenerator(rescale=1./255)\n","datagen.fit(X_train)"]},{"cell_type":"code","execution_count":11,"id":"bc2c5b20","metadata":{"execution":{"iopub.execute_input":"2023-02-08T22:37:30.071049Z","iopub.status.busy":"2023-02-08T22:37:30.070761Z","iopub.status.idle":"2023-02-08T22:37:30.07591Z","shell.execute_reply":"2023-02-08T22:37:30.075083Z"},"papermill":{"duration":0.016597,"end_time":"2023-02-08T22:37:30.077952","exception":false,"start_time":"2023-02-08T22:37:30.061355","status":"completed"},"tags":[]},"outputs":[],"source":["batch_size = 64"]},{"cell_type":"markdown","id":"20fb4d8d","metadata":{"papermill":{"duration":0.007932,"end_time":"2023-02-08T22:37:30.09403","exception":false,"start_time":"2023-02-08T22:37:30.086098","status":"completed"},"tags":[]},"source":["1. __rescale:__ It normalizes the pixel value by dividing it by 255.\n","2. __horizontal_flip:__ It flips the image horizontally.\n","3. __fill_mode:__ It fills the image if not available after some cropping.\n","4. __rotation_range:__ It rotates the image by 0–90 degrees.\n","\n","On testing data, we will only apply rescaling(normalization)."]},{"cell_type":"markdown","id":"5c63b39b","metadata":{"papermill":{"duration":0.00811,"end_time":"2023-02-08T22:37:30.110412","exception":false,"start_time":"2023-02-08T22:37:30.102302","status":"completed"},"tags":[]},"source":["### Fitting the generator to the data\n","\n","We will use batch_size of 64 and after fitting our data to our image generator, data will be generated in the batch size of 64. Using a data generator is the best way to train a large amount of data."]},{"cell_type":"code","execution_count":12,"id":"e5aa801e","metadata":{"execution":{"iopub.execute_input":"2023-02-08T22:37:30.127949Z","iopub.status.busy":"2023-02-08T22:37:30.127681Z","iopub.status.idle":"2023-02-08T22:37:30.164685Z","shell.execute_reply":"2023-02-08T22:37:30.16387Z"},"papermill":{"duration":0.047913,"end_time":"2023-02-08T22:37:30.166636","exception":false,"start_time":"2023-02-08T22:37:30.118723","status":"completed"},"tags":[]},"outputs":[],"source":["train_flow = datagen.flow(X_train, y_train, batch_size=batch_size) \n","test_flow = testgen.flow(X_test, y_test, batch_size=batch_size)"]},{"cell_type":"markdown","id":"a4dd63f6","metadata":{"papermill":{"duration":0.007901,"end_time":"2023-02-08T22:37:30.182783","exception":false,"start_time":"2023-02-08T22:37:30.174882","status":"completed"},"tags":[]},"source":["train_flow contains our X_train and y_train while test_flow contains our X_test and y_test."]},{"cell_type":"markdown","id":"00f139a5","metadata":{"papermill":{"duration":0.007931,"end_time":"2023-02-08T22:37:30.198925","exception":false,"start_time":"2023-02-08T22:37:30.190994","status":"completed"},"tags":[]},"source":["### Building Facial Emotion Detection Model using CNN\n","\n","We are creating blocks using Conv2D layer, Batch-Normalization, Max-Pooling2D, Dropout, Flatten, and then stacking them together and at the end-use Dense Layer for output"]},{"cell_type":"markdown","id":"dba62348","metadata":{"papermill":{"duration":0.007927,"end_time":"2023-02-08T22:37:30.21511","exception":false,"start_time":"2023-02-08T22:37:30.207183","status":"completed"},"tags":[]},"source":["FER_model takes input size and returns model for training. "]},{"cell_type":"code","execution_count":13,"id":"fc3e6ca0","metadata":{"execution":{"iopub.execute_input":"2023-02-08T22:37:30.232902Z","iopub.status.busy":"2023-02-08T22:37:30.232651Z","iopub.status.idle":"2023-02-08T22:37:30.249115Z","shell.execute_reply":"2023-02-08T22:37:30.248229Z"},"papermill":{"duration":0.027648,"end_time":"2023-02-08T22:37:30.250984","exception":false,"start_time":"2023-02-08T22:37:30.223336","status":"completed"},"tags":[]},"outputs":[],"source":["def FER_Model(input_shape=(48,48,1)):\n","    # first input model\n","    visible = Input(shape=input_shape, name='input')\n","    num_classes = 7\n","    #the 1-st block\n","    conv1_1 = Conv2D(64, kernel_size=3, activation='relu', padding='same', name = 'conv1_1')(visible)\n","    conv1_1 = BatchNormalization()(conv1_1)\n","    conv1_2 = Conv2D(64, kernel_size=3, activation='relu', padding='same', name = 'conv1_2')(conv1_1)\n","    conv1_2 = BatchNormalization()(conv1_2)\n","    pool1_1 = MaxPooling2D(pool_size=(2,2), name = 'pool1_1')(conv1_2)\n","    drop1_1 = Dropout(0.3, name = 'drop1_1')(pool1_1)#the 2-nd block\n","    conv2_1 = Conv2D(128, kernel_size=3, activation='relu', padding='same', name = 'conv2_1')(drop1_1)\n","    conv2_1 = BatchNormalization()(conv2_1)\n","    conv2_2 = Conv2D(128, kernel_size=3, activation='relu', padding='same', name = 'conv2_2')(conv2_1)\n","    conv2_2 = BatchNormalization()(conv2_2)\n","    conv2_3 = Conv2D(128, kernel_size=3, activation='relu', padding='same', name = 'conv2_3')(conv2_2)\n","    conv2_2 = BatchNormalization()(conv2_3)\n","    pool2_1 = MaxPooling2D(pool_size=(2,2), name = 'pool2_1')(conv2_3)\n","    drop2_1 = Dropout(0.3, name = 'drop2_1')(pool2_1)#the 3-rd block\n","    conv3_1 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv3_1')(drop2_1)\n","    conv3_1 = BatchNormalization()(conv3_1)\n","    conv3_2 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv3_2')(conv3_1)\n","    conv3_2 = BatchNormalization()(conv3_2)\n","    conv3_3 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv3_3')(conv3_2)\n","    conv3_3 = BatchNormalization()(conv3_3)\n","    conv3_4 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv3_4')(conv3_3)\n","    conv3_4 = BatchNormalization()(conv3_4)\n","    pool3_1 = MaxPooling2D(pool_size=(2,2), name = 'pool3_1')(conv3_4)\n","    drop3_1 = Dropout(0.3, name = 'drop3_1')(pool3_1)#the 4-th block\n","    conv4_1 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv4_1')(drop3_1)\n","    conv4_1 = BatchNormalization()(conv4_1)\n","    conv4_2 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv4_2')(conv4_1)\n","    conv4_2 = BatchNormalization()(conv4_2)\n","    conv4_3 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv4_3')(conv4_2)\n","    conv4_3 = BatchNormalization()(conv4_3)\n","    conv4_4 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv4_4')(conv4_3)\n","    conv4_4 = BatchNormalization()(conv4_4)\n","    pool4_1 = MaxPooling2D(pool_size=(2,2), name = 'pool4_1')(conv4_4)\n","    drop4_1 = Dropout(0.3, name = 'drop4_1')(pool4_1)\n","    \n","    #the 5-th block\n","    conv5_1 = Conv2D(512, kernel_size=3, activation='relu', padding='same', name = 'conv5_1')(drop4_1)\n","    conv5_1 = BatchNormalization()(conv5_1)\n","    conv5_2 = Conv2D(512, kernel_size=3, activation='relu', padding='same', name = 'conv5_2')(conv5_1)\n","    conv5_2 = BatchNormalization()(conv5_2)\n","    conv5_3 = Conv2D(512, kernel_size=3, activation='relu', padding='same', name = 'conv5_3')(conv5_2)\n","    conv5_3 = BatchNormalization()(conv5_3)\n","    conv5_4 = Conv2D(512, kernel_size=3, activation='relu', padding='same', name = 'conv5_4')(conv5_3)\n","    conv5_3 = BatchNormalization()(conv5_3)\n","    pool5_1 = MaxPooling2D(pool_size=(2,2), name = 'pool5_1')(conv5_4)\n","    drop5_1 = Dropout(0.3, name = 'drop5_1')(pool5_1)#Flatten and output\n","    flatten = Flatten(name = 'flatten')(drop5_1)\n","    ouput = Dense(num_classes, activation='softmax', name = 'output')(flatten)# create model \n","    model = Model(inputs =visible, outputs = ouput)\n","    # summary layers\n","    print(model.summary())\n","    \n","    return model"]},{"cell_type":"markdown","id":"a103c9ce","metadata":{"papermill":{"duration":0.007943,"end_time":"2023-02-08T22:37:30.267041","exception":false,"start_time":"2023-02-08T22:37:30.259098","status":"completed"},"tags":[]},"source":["### Compiling model using Adam optimizer"]},{"cell_type":"code","execution_count":14,"id":"f4dc5ca5","metadata":{"execution":{"iopub.execute_input":"2023-02-08T22:37:30.284844Z","iopub.status.busy":"2023-02-08T22:37:30.284036Z","iopub.status.idle":"2023-02-08T22:37:32.968456Z","shell.execute_reply":"2023-02-08T22:37:32.967077Z"},"papermill":{"duration":2.695981,"end_time":"2023-02-08T22:37:32.971172","exception":false,"start_time":"2023-02-08T22:37:30.275191","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-02-08 22:37:30.374825: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2023-02-08 22:37:30.449951: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2023-02-08 22:37:30.450794: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2023-02-08 22:37:30.452701: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-02-08 22:37:30.458479: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2023-02-08 22:37:30.459186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2023-02-08 22:37:30.459869: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2023-02-08 22:37:32.307665: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2023-02-08 22:37:32.308504: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2023-02-08 22:37:32.309171: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2023-02-08 22:37:32.309789: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15401 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"]},{"name":"stdout","output_type":"stream","text":["Model: \"model\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input (InputLayer)           [(None, 48, 48, 1)]       0         \n","_________________________________________________________________\n","conv1_1 (Conv2D)             (None, 48, 48, 64)        640       \n","_________________________________________________________________\n","batch_normalization (BatchNo (None, 48, 48, 64)        256       \n","_________________________________________________________________\n","conv1_2 (Conv2D)             (None, 48, 48, 64)        36928     \n","_________________________________________________________________\n","batch_normalization_1 (Batch (None, 48, 48, 64)        256       \n","_________________________________________________________________\n","pool1_1 (MaxPooling2D)       (None, 24, 24, 64)        0         \n","_________________________________________________________________\n","drop1_1 (Dropout)            (None, 24, 24, 64)        0         \n","_________________________________________________________________\n","conv2_1 (Conv2D)             (None, 24, 24, 128)       73856     \n","_________________________________________________________________\n","batch_normalization_2 (Batch (None, 24, 24, 128)       512       \n","_________________________________________________________________\n","conv2_2 (Conv2D)             (None, 24, 24, 128)       147584    \n","_________________________________________________________________\n","batch_normalization_3 (Batch (None, 24, 24, 128)       512       \n","_________________________________________________________________\n","conv2_3 (Conv2D)             (None, 24, 24, 128)       147584    \n","_________________________________________________________________\n","pool2_1 (MaxPooling2D)       (None, 12, 12, 128)       0         \n","_________________________________________________________________\n","drop2_1 (Dropout)            (None, 12, 12, 128)       0         \n","_________________________________________________________________\n","conv3_1 (Conv2D)             (None, 12, 12, 256)       295168    \n","_________________________________________________________________\n","batch_normalization_5 (Batch (None, 12, 12, 256)       1024      \n","_________________________________________________________________\n","conv3_2 (Conv2D)             (None, 12, 12, 256)       590080    \n","_________________________________________________________________\n","batch_normalization_6 (Batch (None, 12, 12, 256)       1024      \n","_________________________________________________________________\n","conv3_3 (Conv2D)             (None, 12, 12, 256)       590080    \n","_________________________________________________________________\n","batch_normalization_7 (Batch (None, 12, 12, 256)       1024      \n","_________________________________________________________________\n","conv3_4 (Conv2D)             (None, 12, 12, 256)       590080    \n","_________________________________________________________________\n","batch_normalization_8 (Batch (None, 12, 12, 256)       1024      \n","_________________________________________________________________\n","pool3_1 (MaxPooling2D)       (None, 6, 6, 256)         0         \n","_________________________________________________________________\n","drop3_1 (Dropout)            (None, 6, 6, 256)         0         \n","_________________________________________________________________\n","conv4_1 (Conv2D)             (None, 6, 6, 256)         590080    \n","_________________________________________________________________\n","batch_normalization_9 (Batch (None, 6, 6, 256)         1024      \n","_________________________________________________________________\n","conv4_2 (Conv2D)             (None, 6, 6, 256)         590080    \n","_________________________________________________________________\n","batch_normalization_10 (Batc (None, 6, 6, 256)         1024      \n","_________________________________________________________________\n","conv4_3 (Conv2D)             (None, 6, 6, 256)         590080    \n","_________________________________________________________________\n","batch_normalization_11 (Batc (None, 6, 6, 256)         1024      \n","_________________________________________________________________\n","conv4_4 (Conv2D)             (None, 6, 6, 256)         590080    \n","_________________________________________________________________\n","batch_normalization_12 (Batc (None, 6, 6, 256)         1024      \n","_________________________________________________________________\n","pool4_1 (MaxPooling2D)       (None, 3, 3, 256)         0         \n","_________________________________________________________________\n","drop4_1 (Dropout)            (None, 3, 3, 256)         0         \n","_________________________________________________________________\n","conv5_1 (Conv2D)             (None, 3, 3, 512)         1180160   \n","_________________________________________________________________\n","batch_normalization_13 (Batc (None, 3, 3, 512)         2048      \n","_________________________________________________________________\n","conv5_2 (Conv2D)             (None, 3, 3, 512)         2359808   \n","_________________________________________________________________\n","batch_normalization_14 (Batc (None, 3, 3, 512)         2048      \n","_________________________________________________________________\n","conv5_3 (Conv2D)             (None, 3, 3, 512)         2359808   \n","_________________________________________________________________\n","batch_normalization_15 (Batc (None, 3, 3, 512)         2048      \n","_________________________________________________________________\n","conv5_4 (Conv2D)             (None, 3, 3, 512)         2359808   \n","_________________________________________________________________\n","pool5_1 (MaxPooling2D)       (None, 1, 1, 512)         0         \n","_________________________________________________________________\n","drop5_1 (Dropout)            (None, 1, 1, 512)         0         \n","_________________________________________________________________\n","flatten (Flatten)            (None, 512)               0         \n","_________________________________________________________________\n","output (Dense)               (None, 7)                 3591      \n","=================================================================\n","Total params: 13,111,367\n","Trainable params: 13,103,431\n","Non-trainable params: 7,936\n","_________________________________________________________________\n","None\n"]}],"source":["model = FER_Model()\n","opt = Adam(learning_rate=0.0001, decay=1e-6)\n","model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])"]},{"cell_type":"markdown","id":"c8b97bb2","metadata":{"papermill":{"duration":0.008491,"end_time":"2023-02-08T22:37:32.988724","exception":false,"start_time":"2023-02-08T22:37:32.980233","status":"completed"},"tags":[]},"source":["### Training the Facial Emotion Detection Model"]},{"cell_type":"code","execution_count":15,"id":"804b0d0f","metadata":{"execution":{"iopub.execute_input":"2023-02-08T22:37:33.007579Z","iopub.status.busy":"2023-02-08T22:37:33.006846Z","iopub.status.idle":"2023-02-08T23:10:18.11798Z","shell.execute_reply":"2023-02-08T23:10:18.116943Z"},"papermill":{"duration":1965.123009,"end_time":"2023-02-08T23:10:18.120235","exception":false,"start_time":"2023-02-08T22:37:32.997226","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/100\n"]},{"name":"stderr","output_type":"stream","text":["2023-02-08 22:37:33.096561: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n","2023-02-08 22:37:35.439784: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8005\n"]},{"name":"stdout","output_type":"stream","text":["449/449 [==============================] - 28s 44ms/step - loss: 2.0285 - accuracy: 0.2180 - val_loss: 1.8459 - val_accuracy: 0.2497\n","Epoch 2/100\n","449/449 [==============================] - 19s 43ms/step - loss: 1.7889 - accuracy: 0.2569 - val_loss: 1.8368 - val_accuracy: 0.2756\n","Epoch 3/100\n","449/449 [==============================] - 19s 42ms/step - loss: 1.7393 - accuracy: 0.2906 - val_loss: 1.8914 - val_accuracy: 0.3004\n","Epoch 4/100\n","449/449 [==============================] - 19s 43ms/step - loss: 1.6844 - accuracy: 0.3281 - val_loss: 1.6824 - val_accuracy: 0.3711\n","Epoch 5/100\n","449/449 [==============================] - 19s 43ms/step - loss: 1.6137 - accuracy: 0.3633 - val_loss: 1.5190 - val_accuracy: 0.4185\n","Epoch 6/100\n","449/449 [==============================] - 19s 42ms/step - loss: 1.5435 - accuracy: 0.3998 - val_loss: 1.5755 - val_accuracy: 0.4023\n","Epoch 7/100\n","449/449 [==============================] - 19s 43ms/step - loss: 1.4812 - accuracy: 0.4259 - val_loss: 1.5696 - val_accuracy: 0.4283\n","Epoch 8/100\n","449/449 [==============================] - 19s 42ms/step - loss: 1.4185 - accuracy: 0.4563 - val_loss: 1.4285 - val_accuracy: 0.4653\n","Epoch 9/100\n","449/449 [==============================] - 19s 43ms/step - loss: 1.3592 - accuracy: 0.4819 - val_loss: 1.3498 - val_accuracy: 0.4909\n","Epoch 10/100\n","449/449 [==============================] - 20s 44ms/step - loss: 1.3056 - accuracy: 0.5041 - val_loss: 1.2540 - val_accuracy: 0.5230\n","Epoch 11/100\n","449/449 [==============================] - 19s 43ms/step - loss: 1.2625 - accuracy: 0.5173 - val_loss: 1.2522 - val_accuracy: 0.5439\n","Epoch 12/100\n","449/449 [==============================] - 19s 43ms/step - loss: 1.2245 - accuracy: 0.5360 - val_loss: 1.2107 - val_accuracy: 0.5469\n","Epoch 13/100\n","449/449 [==============================] - 19s 43ms/step - loss: 1.1962 - accuracy: 0.5485 - val_loss: 1.1838 - val_accuracy: 0.5486\n","Epoch 14/100\n","449/449 [==============================] - 20s 44ms/step - loss: 1.1641 - accuracy: 0.5598 - val_loss: 1.2276 - val_accuracy: 0.5467\n","Epoch 15/100\n","449/449 [==============================] - 20s 44ms/step - loss: 1.1401 - accuracy: 0.5725 - val_loss: 1.1011 - val_accuracy: 0.5818\n","Epoch 16/100\n","449/449 [==============================] - 19s 43ms/step - loss: 1.1150 - accuracy: 0.5809 - val_loss: 1.1434 - val_accuracy: 0.5804\n","Epoch 17/100\n","449/449 [==============================] - 20s 43ms/step - loss: 1.0865 - accuracy: 0.5892 - val_loss: 1.1057 - val_accuracy: 0.5938\n","Epoch 18/100\n","449/449 [==============================] - 19s 43ms/step - loss: 1.0705 - accuracy: 0.5976 - val_loss: 1.0523 - val_accuracy: 0.6102\n","Epoch 19/100\n","449/449 [==============================] - 20s 44ms/step - loss: 1.0523 - accuracy: 0.6045 - val_loss: 1.0704 - val_accuracy: 0.5957\n","Epoch 20/100\n","449/449 [==============================] - 20s 44ms/step - loss: 1.0333 - accuracy: 0.6096 - val_loss: 1.1224 - val_accuracy: 0.5893\n","Epoch 21/100\n","449/449 [==============================] - 19s 42ms/step - loss: 1.0212 - accuracy: 0.6166 - val_loss: 1.0534 - val_accuracy: 0.6119\n","Epoch 22/100\n","449/449 [==============================] - 20s 44ms/step - loss: 0.9991 - accuracy: 0.6243 - val_loss: 1.0325 - val_accuracy: 0.6208\n","Epoch 23/100\n","449/449 [==============================] - 20s 43ms/step - loss: 0.9847 - accuracy: 0.6299 - val_loss: 1.0170 - val_accuracy: 0.6300\n","Epoch 24/100\n","449/449 [==============================] - 19s 43ms/step - loss: 0.9742 - accuracy: 0.6329 - val_loss: 1.0396 - val_accuracy: 0.6158\n","Epoch 25/100\n","449/449 [==============================] - 19s 43ms/step - loss: 0.9570 - accuracy: 0.6392 - val_loss: 1.0509 - val_accuracy: 0.6149\n","Epoch 26/100\n","449/449 [==============================] - 19s 42ms/step - loss: 0.9430 - accuracy: 0.6481 - val_loss: 0.9733 - val_accuracy: 0.6397\n","Epoch 27/100\n","449/449 [==============================] - 20s 44ms/step - loss: 0.9292 - accuracy: 0.6512 - val_loss: 1.0192 - val_accuracy: 0.6225\n","Epoch 28/100\n","449/449 [==============================] - 20s 44ms/step - loss: 0.9169 - accuracy: 0.6562 - val_loss: 0.9995 - val_accuracy: 0.6445\n","Epoch 29/100\n","449/449 [==============================] - 19s 42ms/step - loss: 0.9026 - accuracy: 0.6634 - val_loss: 1.0432 - val_accuracy: 0.6147\n","Epoch 30/100\n","449/449 [==============================] - 20s 44ms/step - loss: 0.8887 - accuracy: 0.6656 - val_loss: 0.9897 - val_accuracy: 0.6434\n","Epoch 31/100\n","449/449 [==============================] - 19s 42ms/step - loss: 0.8755 - accuracy: 0.6725 - val_loss: 0.9746 - val_accuracy: 0.6459\n","Epoch 32/100\n","449/449 [==============================] - 20s 44ms/step - loss: 0.8618 - accuracy: 0.6763 - val_loss: 0.9841 - val_accuracy: 0.6447\n","Epoch 33/100\n","449/449 [==============================] - 20s 44ms/step - loss: 0.8572 - accuracy: 0.6794 - val_loss: 0.9876 - val_accuracy: 0.6383\n","Epoch 34/100\n","449/449 [==============================] - 19s 42ms/step - loss: 0.8430 - accuracy: 0.6866 - val_loss: 0.9510 - val_accuracy: 0.6478\n","Epoch 35/100\n","449/449 [==============================] - 20s 44ms/step - loss: 0.8294 - accuracy: 0.6877 - val_loss: 0.9803 - val_accuracy: 0.6486\n","Epoch 36/100\n","449/449 [==============================] - 19s 43ms/step - loss: 0.8178 - accuracy: 0.6936 - val_loss: 1.0023 - val_accuracy: 0.6484\n","Epoch 37/100\n","449/449 [==============================] - 19s 43ms/step - loss: 0.8086 - accuracy: 0.6974 - val_loss: 0.9641 - val_accuracy: 0.6525\n","Epoch 38/100\n","449/449 [==============================] - 20s 44ms/step - loss: 0.7908 - accuracy: 0.7060 - val_loss: 0.9645 - val_accuracy: 0.6601\n","Epoch 39/100\n","449/449 [==============================] - 19s 42ms/step - loss: 0.7890 - accuracy: 0.7069 - val_loss: 1.0886 - val_accuracy: 0.6236\n","Epoch 40/100\n","449/449 [==============================] - 20s 44ms/step - loss: 0.7797 - accuracy: 0.7091 - val_loss: 0.9766 - val_accuracy: 0.6612\n","Epoch 41/100\n","449/449 [==============================] - 19s 43ms/step - loss: 0.7652 - accuracy: 0.7151 - val_loss: 0.9778 - val_accuracy: 0.6548\n","Epoch 42/100\n","449/449 [==============================] - 19s 43ms/step - loss: 0.7553 - accuracy: 0.7144 - val_loss: 0.9893 - val_accuracy: 0.6548\n","Epoch 43/100\n","449/449 [==============================] - 20s 44ms/step - loss: 0.7428 - accuracy: 0.7226 - val_loss: 0.9618 - val_accuracy: 0.6598\n","Epoch 44/100\n","449/449 [==============================] - 19s 43ms/step - loss: 0.7292 - accuracy: 0.7278 - val_loss: 1.0053 - val_accuracy: 0.6595\n","Epoch 45/100\n","449/449 [==============================] - 19s 43ms/step - loss: 0.7166 - accuracy: 0.7319 - val_loss: 1.0089 - val_accuracy: 0.6559\n","Epoch 46/100\n","449/449 [==============================] - 20s 44ms/step - loss: 0.7024 - accuracy: 0.7377 - val_loss: 1.0279 - val_accuracy: 0.6556\n","Epoch 47/100\n","449/449 [==============================] - 19s 42ms/step - loss: 0.6934 - accuracy: 0.7409 - val_loss: 1.0112 - val_accuracy: 0.6584\n","Epoch 48/100\n","449/449 [==============================] - 20s 45ms/step - loss: 0.6913 - accuracy: 0.7428 - val_loss: 1.0329 - val_accuracy: 0.6534\n","Epoch 49/100\n","449/449 [==============================] - 20s 45ms/step - loss: 0.6778 - accuracy: 0.7482 - val_loss: 1.0056 - val_accuracy: 0.6581\n","Epoch 50/100\n","449/449 [==============================] - 20s 45ms/step - loss: 0.6673 - accuracy: 0.7529 - val_loss: 0.9887 - val_accuracy: 0.6595\n","Epoch 51/100\n","449/449 [==============================] - 20s 45ms/step - loss: 0.6642 - accuracy: 0.7521 - val_loss: 1.0199 - val_accuracy: 0.6707\n","Epoch 52/100\n","449/449 [==============================] - 19s 43ms/step - loss: 0.6487 - accuracy: 0.7576 - val_loss: 1.0280 - val_accuracy: 0.6643\n","Epoch 53/100\n","449/449 [==============================] - 20s 44ms/step - loss: 0.6343 - accuracy: 0.7641 - val_loss: 1.0000 - val_accuracy: 0.6676\n","Epoch 54/100\n","449/449 [==============================] - 20s 43ms/step - loss: 0.6314 - accuracy: 0.7645 - val_loss: 0.9998 - val_accuracy: 0.6737\n","Epoch 55/100\n","449/449 [==============================] - 19s 43ms/step - loss: 0.6153 - accuracy: 0.7696 - val_loss: 1.0552 - val_accuracy: 0.6598\n","Epoch 56/100\n","449/449 [==============================] - 20s 44ms/step - loss: 0.6042 - accuracy: 0.7775 - val_loss: 1.0643 - val_accuracy: 0.6595\n","Epoch 57/100\n","449/449 [==============================] - 19s 42ms/step - loss: 0.5987 - accuracy: 0.7779 - val_loss: 1.0169 - val_accuracy: 0.6629\n","Epoch 58/100\n","449/449 [==============================] - 19s 43ms/step - loss: 0.5868 - accuracy: 0.7811 - val_loss: 1.0384 - val_accuracy: 0.6659\n","Epoch 59/100\n","449/449 [==============================] - 20s 43ms/step - loss: 0.5771 - accuracy: 0.7851 - val_loss: 1.0313 - val_accuracy: 0.6656\n","Epoch 60/100\n","449/449 [==============================] - 19s 42ms/step - loss: 0.5660 - accuracy: 0.7911 - val_loss: 1.1086 - val_accuracy: 0.6615\n","Epoch 61/100\n","449/449 [==============================] - 20s 44ms/step - loss: 0.5694 - accuracy: 0.7847 - val_loss: 1.0946 - val_accuracy: 0.6626\n","Epoch 62/100\n","449/449 [==============================] - 19s 43ms/step - loss: 0.5521 - accuracy: 0.7955 - val_loss: 1.0413 - val_accuracy: 0.6704\n","Epoch 63/100\n","449/449 [==============================] - 20s 43ms/step - loss: 0.5461 - accuracy: 0.7941 - val_loss: 1.1079 - val_accuracy: 0.6651\n","Epoch 64/100\n","449/449 [==============================] - 20s 45ms/step - loss: 0.5370 - accuracy: 0.8008 - val_loss: 1.0564 - val_accuracy: 0.6737\n","Epoch 65/100\n","449/449 [==============================] - 19s 43ms/step - loss: 0.5279 - accuracy: 0.8039 - val_loss: 1.0609 - val_accuracy: 0.6838\n","Epoch 66/100\n","449/449 [==============================] - 19s 43ms/step - loss: 0.5220 - accuracy: 0.8056 - val_loss: 1.0890 - val_accuracy: 0.6721\n","Epoch 67/100\n","449/449 [==============================] - 19s 43ms/step - loss: 0.5119 - accuracy: 0.8095 - val_loss: 1.1916 - val_accuracy: 0.6587\n","Epoch 68/100\n","449/449 [==============================] - 20s 44ms/step - loss: 0.5092 - accuracy: 0.8109 - val_loss: 1.1174 - val_accuracy: 0.6684\n","Epoch 69/100\n","449/449 [==============================] - 20s 45ms/step - loss: 0.4995 - accuracy: 0.8147 - val_loss: 1.1571 - val_accuracy: 0.6704\n","Epoch 70/100\n","449/449 [==============================] - 19s 43ms/step - loss: 0.4930 - accuracy: 0.8168 - val_loss: 1.1112 - val_accuracy: 0.6754\n","Epoch 71/100\n","449/449 [==============================] - 19s 43ms/step - loss: 0.4781 - accuracy: 0.8243 - val_loss: 1.1424 - val_accuracy: 0.6682\n","Epoch 72/100\n","449/449 [==============================] - 19s 43ms/step - loss: 0.4755 - accuracy: 0.8243 - val_loss: 1.1941 - val_accuracy: 0.6656\n","Epoch 73/100\n","449/449 [==============================] - 19s 42ms/step - loss: 0.4662 - accuracy: 0.8232 - val_loss: 1.1308 - val_accuracy: 0.6796\n","Epoch 74/100\n","449/449 [==============================] - 20s 45ms/step - loss: 0.4571 - accuracy: 0.8314 - val_loss: 1.1648 - val_accuracy: 0.6662\n","Epoch 75/100\n","449/449 [==============================] - 19s 43ms/step - loss: 0.4493 - accuracy: 0.8332 - val_loss: 1.1351 - val_accuracy: 0.6776\n","Epoch 76/100\n","449/449 [==============================] - 19s 43ms/step - loss: 0.4505 - accuracy: 0.8322 - val_loss: 1.1301 - val_accuracy: 0.6799\n","Epoch 77/100\n","449/449 [==============================] - 20s 44ms/step - loss: 0.4382 - accuracy: 0.8387 - val_loss: 1.2511 - val_accuracy: 0.6634\n","Epoch 78/100\n","449/449 [==============================] - 19s 42ms/step - loss: 0.4274 - accuracy: 0.8426 - val_loss: 1.2401 - val_accuracy: 0.6623\n","Epoch 79/100\n","449/449 [==============================] - 20s 45ms/step - loss: 0.4309 - accuracy: 0.8388 - val_loss: 1.1906 - val_accuracy: 0.6779\n","Epoch 80/100\n","449/449 [==============================] - 19s 43ms/step - loss: 0.4185 - accuracy: 0.8444 - val_loss: 1.2357 - val_accuracy: 0.6888\n","Epoch 81/100\n","449/449 [==============================] - 20s 45ms/step - loss: 0.4093 - accuracy: 0.8498 - val_loss: 1.2059 - val_accuracy: 0.6748\n","Epoch 82/100\n","449/449 [==============================] - 21s 46ms/step - loss: 0.4044 - accuracy: 0.8506 - val_loss: 1.2196 - val_accuracy: 0.6751\n","Epoch 83/100\n","449/449 [==============================] - 19s 43ms/step - loss: 0.4006 - accuracy: 0.8523 - val_loss: 1.2187 - val_accuracy: 0.6726\n","Epoch 84/100\n","449/449 [==============================] - 20s 44ms/step - loss: 0.3937 - accuracy: 0.8543 - val_loss: 1.2205 - val_accuracy: 0.6782\n","Epoch 85/100\n","449/449 [==============================] - 20s 44ms/step - loss: 0.3897 - accuracy: 0.8562 - val_loss: 1.2821 - val_accuracy: 0.6726\n","Epoch 86/100\n","449/449 [==============================] - 19s 42ms/step - loss: 0.3818 - accuracy: 0.8577 - val_loss: 1.2149 - val_accuracy: 0.6734\n","Epoch 87/100\n","449/449 [==============================] - 20s 45ms/step - loss: 0.3661 - accuracy: 0.8638 - val_loss: 1.2437 - val_accuracy: 0.6729\n","Epoch 88/100\n","449/449 [==============================] - 20s 44ms/step - loss: 0.3691 - accuracy: 0.8624 - val_loss: 1.2401 - val_accuracy: 0.6838\n","Epoch 89/100\n","449/449 [==============================] - 19s 43ms/step - loss: 0.3607 - accuracy: 0.8671 - val_loss: 1.3281 - val_accuracy: 0.6595\n","Epoch 90/100\n","449/449 [==============================] - 20s 44ms/step - loss: 0.3514 - accuracy: 0.8676 - val_loss: 1.3831 - val_accuracy: 0.6620\n","Epoch 91/100\n","449/449 [==============================] - 19s 42ms/step - loss: 0.3602 - accuracy: 0.8686 - val_loss: 1.2898 - val_accuracy: 0.6871\n","Epoch 92/100\n","449/449 [==============================] - 20s 45ms/step - loss: 0.3485 - accuracy: 0.8697 - val_loss: 1.3253 - val_accuracy: 0.6732\n","Epoch 93/100\n","449/449 [==============================] - 20s 43ms/step - loss: 0.3450 - accuracy: 0.8718 - val_loss: 1.3048 - val_accuracy: 0.6773\n","Epoch 94/100\n","449/449 [==============================] - 19s 43ms/step - loss: 0.3315 - accuracy: 0.8766 - val_loss: 1.2941 - val_accuracy: 0.6810\n","Epoch 95/100\n","449/449 [==============================] - 20s 45ms/step - loss: 0.3348 - accuracy: 0.8739 - val_loss: 1.2904 - val_accuracy: 0.6824\n","Epoch 96/100\n","449/449 [==============================] - 19s 42ms/step - loss: 0.3322 - accuracy: 0.8779 - val_loss: 1.4240 - val_accuracy: 0.6804\n","Epoch 97/100\n","449/449 [==============================] - 20s 45ms/step - loss: 0.3229 - accuracy: 0.8808 - val_loss: 1.3736 - val_accuracy: 0.6815\n","Epoch 98/100\n","449/449 [==============================] - 19s 42ms/step - loss: 0.3216 - accuracy: 0.8812 - val_loss: 1.3301 - val_accuracy: 0.6868\n","Epoch 99/100\n","449/449 [==============================] - 20s 44ms/step - loss: 0.3108 - accuracy: 0.8832 - val_loss: 1.3654 - val_accuracy: 0.6807\n","Epoch 100/100\n","449/449 [==============================] - 20s 45ms/step - loss: 0.3151 - accuracy: 0.8834 - val_loss: 1.4258 - val_accuracy: 0.6782\n"]}],"source":["num_epochs = 100  \n","history = model.fit(train_flow,  \n","                    epochs=num_epochs,  \n","                    verbose=1,  \n","                    validation_data=test_flow)"]},{"cell_type":"markdown","id":"f1028d27","metadata":{"papermill":{"duration":1.508563,"end_time":"2023-02-08T23:10:20.952749","exception":false,"start_time":"2023-02-08T23:10:19.444186","status":"completed"},"tags":[]},"source":["validation_steps=len(X_test) / batch_size)\n","\n","1. __steps_per_epoch__ = TotalTrainingSamples / TrainingBatchSize\n","2. __validation_steps__ = TotalvalidationSamples / ValidationBatchSize\n","\n","Training takes at least 20 minutes for 100 epochs."]},{"cell_type":"markdown","id":"67ba617a","metadata":{"papermill":{"duration":1.319457,"end_time":"2023-02-08T23:10:23.632259","exception":false,"start_time":"2023-02-08T23:10:22.312802","status":"completed"},"tags":[]},"source":["### Saving our model’s architecture into JSON and model’s weight into .h5."]},{"cell_type":"code","execution_count":16,"id":"472eeb40","metadata":{"execution":{"iopub.execute_input":"2023-02-08T23:10:26.299598Z","iopub.status.busy":"2023-02-08T23:10:26.299045Z","iopub.status.idle":"2023-02-08T23:10:26.454883Z","shell.execute_reply":"2023-02-08T23:10:26.452835Z"},"papermill":{"duration":1.48003,"end_time":"2023-02-08T23:10:26.457949","exception":false,"start_time":"2023-02-08T23:10:24.977919","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Saved model to disk\n"]}],"source":["model_json = model.to_json()\n","with open(\"model.json\", \"w\") as json_file:\n","    json_file.write(model_json)\n","model.save_weights(\"model.h5\")\n","print(\"Saved model to disk\")"]},{"cell_type":"markdown","id":"5758ae0d","metadata":{"papermill":{"duration":1.312691,"end_time":"2023-02-08T23:10:29.071869","exception":false,"start_time":"2023-02-08T23:10:27.759178","status":"completed"},"tags":[]},"source":["### Dumping the ML model into a pickle:\n","\n","In Python, pickling refers to storing a variable into a **.pkl** file."]},{"cell_type":"code","execution_count":17,"id":"44346b11","metadata":{"execution":{"iopub.execute_input":"2023-02-08T23:10:31.848164Z","iopub.status.busy":"2023-02-08T23:10:31.847725Z","iopub.status.idle":"2023-02-08T23:10:31.852879Z","shell.execute_reply":"2023-02-08T23:10:31.851709Z"},"papermill":{"duration":1.452385,"end_time":"2023-02-08T23:10:31.856146","exception":false,"start_time":"2023-02-08T23:10:30.403761","status":"completed"},"tags":[]},"outputs":[],"source":["#joblib.dump(model, \"clf.pkl\")"]},{"cell_type":"markdown","id":"98b6a66b","metadata":{"papermill":{"duration":1.258897,"end_time":"2023-02-08T23:10:34.569314","exception":false,"start_time":"2023-02-08T23:10:33.310417","status":"completed"},"tags":[]},"source":["### Testing the Model using Webcam Feed\n","\n","we will test our model in real-time using face detection."]},{"cell_type":"code","execution_count":18,"id":"7519e316","metadata":{"execution":{"iopub.execute_input":"2023-02-08T23:10:37.222472Z","iopub.status.busy":"2023-02-08T23:10:37.222091Z","iopub.status.idle":"2023-02-08T23:10:37.597332Z","shell.execute_reply":"2023-02-08T23:10:37.596114Z"},"papermill":{"duration":1.687895,"end_time":"2023-02-08T23:10:37.600041","exception":false,"start_time":"2023-02-08T23:10:35.912146","status":"completed"},"tags":[]},"outputs":[],"source":["from tensorflow.keras.models import model_from_json\n","model = model_from_json(open(\"model.json\", \"r\").read())\n","model.load_weights('model.h5')"]},{"cell_type":"markdown","id":"aa0c3e8d","metadata":{"papermill":{"duration":1.319245,"end_time":"2023-02-08T23:10:40.22225","exception":false,"start_time":"2023-02-08T23:10:38.903005","status":"completed"},"tags":[]},"source":["### Loading Har-Cascade for Face Detection"]},{"cell_type":"code","execution_count":19,"id":"701252e1","metadata":{"execution":{"iopub.execute_input":"2023-02-08T23:10:43.600097Z","iopub.status.busy":"2023-02-08T23:10:43.599735Z","iopub.status.idle":"2023-02-08T23:10:43.737089Z","shell.execute_reply":"2023-02-08T23:10:43.736128Z"},"papermill":{"duration":2.026774,"end_time":"2023-02-08T23:10:43.739288","exception":false,"start_time":"2023-02-08T23:10:41.712514","status":"completed"},"tags":[]},"outputs":[],"source":["import cv2\n","face_haar_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')"]},{"cell_type":"markdown","id":"3127d5f9","metadata":{"papermill":{"duration":1.341583,"end_time":"2023-02-08T23:10:46.335507","exception":false,"start_time":"2023-02-08T23:10:44.993924","status":"completed"},"tags":[]},"source":["### Read Frames and apply Preprocessing using OpenCV"]},{"cell_type":"code","execution_count":20,"id":"e2aeef00","metadata":{"execution":{"iopub.execute_input":"2023-02-08T23:10:48.952818Z","iopub.status.busy":"2023-02-08T23:10:48.952451Z","iopub.status.idle":"2023-02-08T23:10:48.980842Z","shell.execute_reply":"2023-02-08T23:10:48.979753Z"},"papermill":{"duration":1.342273,"end_time":"2023-02-08T23:10:48.982907","exception":false,"start_time":"2023-02-08T23:10:47.640634","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["[ WARN:0] global /tmp/pip-req-build-jpmv6t9_/opencv/modules/videoio/src/cap_v4l.cpp (890) open VIDEOIO(V4L2:/dev/video1): can't open camera by index\n"]},{"data":{"text/plain":["<function destroyAllWindows>"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["cap=cv2.VideoCapture(1)\n","while cap.isOpened():\n","    res,frame=cap.read()\n","    height, width , channel = frame.shape\n","    # Creating an Overlay window to write prediction and cofidence\n","    sub_img = frame[0:int(height/6),0:int(width)]\n","    black_rect = np.ones(sub_img.shape, dtype=np.uint8)*0\n","    res = cv2.addWeighted(sub_img, 0.77, black_rect,0.23, 0)\n","    FONT = cv2.FONT_HERSHEY_SIMPLEX\n","    FONT_SCALE = 0.8\n","    FONT_THICKNESS = 2\n","    lable_color = (10, 10, 255)\n","    lable = \"Emotion Detection\"\n","    lable_dimension = cv2.getTextSize(lable,FONT ,FONT_SCALE,FONT_THICKNESS)[0]\n","    textX = int((res.shape[1] - lable_dimension[0]) / 2)\n","    textY = int((res.shape[0] + lable_dimension[1]) / 2)\n","    cv2.putText(res, lable, (textX,textY), FONT, FONT_SCALE, (0,0,0), FONT_THICKNESS)# prediction part --------------------------------------------------------------------------\n","    gray_image= cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n","    faces = face_haar_cascade.detectMultiScale(gray_image )\n","    try:\n","        for (x,y, w, h) in faces:\n","            cv2.rectangle(frame, pt1 = (x,y),pt2 = (x+w, y+h), color = (255,0,0),thickness =  2)\n","            roi_gray = gray_image[y-5:y+h+5,x-5:x+w+5]\n","            roi_gray=cv2.resize(roi_gray,(48,48))\n","            image_pixels = img_to_array(roi_gray)\n","            image_pixels = np.expand_dims(image_pixels, axis = 0)\n","            image_pixels /= 255\n","            predictions = model.predict(image_pixels)\n","            max_index = np.argmax(predictions[0])\n","            emotion_detection = ('angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral')\n","            emotion_prediction = emotion_detection[max_index]\n","            cv2.putText(res, \"Sentiment: {}\".format(emotion_prediction), (0,textY+22+5), FONT,0.7, lable_color,2)\n","            lable_violation = 'Confidence: {}'.format(str(np.round(np.max(predictions[0])*100,1))+ \"%\")\n","            violation_text_dimension = cv2.getTextSize(lable_violation,FONT,FONT_SCALE,FONT_THICKNESS )[0]\n","            violation_x_axis = int(res.shape[1]- violation_text_dimension[0])\n","            cv2.putText(res, lable_violation, (violation_x_axis,textY+22+5), FONT,0.7, lable_color,2)\n","    except :\n","        pass\n","    frame[0:int(height/6),0:int(width)] = res\n","    cv2.imshow('frame', frame)\n","    if cv2.waitKey(1) & 0xFF == ord('q'):\n","        breakcap.release()\n","cv2.destroyAllWindows"]},{"cell_type":"code","execution_count":null,"id":"edb0f33a","metadata":{"papermill":{"duration":1.305091,"end_time":"2023-02-08T23:10:51.600216","exception":false,"start_time":"2023-02-08T23:10:50.295125","status":"completed"},"tags":[]},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"e302af0d","metadata":{"papermill":{"duration":1.511343,"end_time":"2023-02-08T23:10:54.432288","exception":false,"start_time":"2023-02-08T23:10:52.920945","status":"completed"},"tags":[]},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"df9e07bd","metadata":{"papermill":{"duration":1.308838,"end_time":"2023-02-08T23:10:57.061531","exception":false,"start_time":"2023-02-08T23:10:55.752693","status":"completed"},"tags":[]},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"c52504e3","metadata":{"papermill":{"duration":1.367527,"end_time":"2023-02-08T23:10:59.796573","exception":false,"start_time":"2023-02-08T23:10:58.429046","status":"completed"},"tags":[]},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"39950877","metadata":{"papermill":{"duration":1.310225,"end_time":"2023-02-08T23:11:02.386162","exception":false,"start_time":"2023-02-08T23:11:01.075937","status":"completed"},"tags":[]},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"38353e28","metadata":{"papermill":{"duration":1.55004,"end_time":"2023-02-08T23:11:05.315942","exception":false,"start_time":"2023-02-08T23:11:03.765902","status":"completed"},"tags":[]},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"066722c0","metadata":{"papermill":{"duration":1.308138,"end_time":"2023-02-08T23:11:07.946625","exception":false,"start_time":"2023-02-08T23:11:06.638487","status":"completed"},"tags":[]},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"b0a5b0e7","metadata":{"papermill":{"duration":1.242874,"end_time":"2023-02-08T23:11:10.553736","exception":false,"start_time":"2023-02-08T23:11:09.310862","status":"completed"},"tags":[]},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"papermill":{"default_parameters":{},"duration":2079.324613,"end_time":"2023-02-08T23:11:14.794948","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-02-08T22:36:35.470335","version":"2.3.4"}},"nbformat":4,"nbformat_minor":5}